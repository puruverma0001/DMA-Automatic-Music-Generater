#!/usr/bin/env python
# coding: utf-8


import sys
from IPython import get_ipython
get_ipython().system('pip install --upgrade music21')
get_ipython().system('pip install --upgrade tensorflow')
get_ipython().system('pip install keras')


import pandas as pfd
import numpy as np
import matplotlib.pyplot as plt
import os
from collections import Counter
from sklearn.model_selection import train_test_split
from keras.layers import *
from keras.models import *
from keras.callbacks import *
import keras.backend as K
from keras.models import load_model
import random

# Library for understanding music (MIDI files in datasets)
from music21 import *


class AutomaticMusicGenerator:
    X_test = None
    y_test = None
    no_of_timesteps = None
    unique_x = None
   
    def __init__(self):
        pass
    
    def read_midi(self,file):
    
        print("Loading Music Files:",file)
        
        music_notes = []
        notes_raw = None
        
        # Parsing MIDI file
        midi = converter.parse(file)
        
        # Grouping based on different instruments
        music_instruments = instrument.partitionByInstrument(midi)
        
        # Traversing over all instruments
        for part in music_instruments.parts:
            
            # Select elements of only piano
            if 'Piano' in str(part):
                notes_raw = part.recurse()
                
                # Finding note and chord for each particular element
                for element in notes_raw:
                    
                    # For notes
                    if isinstance(element, note.Note):
                        music_notes.append(str(element.pitch))
                        
                    # For chords
                    elif isinstance(element, chord.Chord):
                        music_notes.append('.'.join(str(n) for n in element.normalOrder))
                
        
        return np.array(music_notes) 
    
    def lstm(self):
        model = Sequential()
        model.add(LSTM(128,return_sequences=True))
        model.add(LSTM(128))
        model.add(Dense(256))
        model.add(Activation('relu'))
        model.add(Dense(n_vocab))
        model.compile(loss='sparse_categorical_crossentropy',optimizer='adam')
        return model
    
    def convert_to_midi(self,prediction_output):

        offset = 0
        output_notes = []
        
        # Create Note and chords objects based on the values generated by the model
        for pattern in prediction_output:
            
            # Pattern is a chord
            if ('.' in pattern) or pattern.isdigit():
                notes_in_chord = pattern.split('.')
                notes = []
                for current_note in notes_in_chord:
                    
                    cn=int(current_note)
                    new_note = note.Note(cn)
                    new_note.storedInstrument = instrument.Piano()
                    notes.append(new_note)
                    
                new_chord = chord.Chord(notes)
                new_chord.offset = offset
                output_notes.append(new_chord)
                
            # Pattern is note
            else:
                new_note = note.Note(pattern)
                new_note.offset = offset
                new_note.storedInstrument = instrument.Piano()
                output_notes.append(new_note)
                
            # Increase offset each iteration so that notes do not stack
            offset += 1
        midi_stream = stream.Stream(output_notes)
        midi_stream.write('midi',fp="music.mid")
            
    def setupModel(self):
        # Loading Music files
        
        # Loading MIDI files
        
        # Path
        path_to_dataset = 'Data-set/'
        
        # Read all filenames
        files=[i for i in os.listdir(path_to_dataset) if i.endswith(".mid")]
        
        # Reading each midi file
        notes_array = np.array([self.read_midi(path_to_dataset+i) for i in files])
        
        # Understanding the data
        
        notes_array.shape
        
        # Converting 2D array into 1D array
        notes_reshaped = [element for note_ in notes_array for element in note_]
        
        # No. of unique notes
        unique_notes = list(set(notes_reshaped))
        print(len(unique_notes))
        
        # No. of unique notes are mentioned as above (304 in our case)
        
        
        
        # Computing frequency of each note
        freq = dict(Counter(notes_reshaped))
        
        # Counsider only the freqenucies
        no=[count for _,count in freq.items()]
        
        # PLoting a frequency histogam
        plt.figure(figsize=(5,5))
        plt.hist(no,bins=10);
        
        
        # From above plot it is infered that most of the notes have very low frequency.
        # So we can keep top frequency ones.
        # We can keep the threshold as 50
        
        high_frequency_notes = [note_ for note_, count in freq.items() if count>=50]
        print(len(high_frequency_notes))
        
        # Now we have around 170 notes. So we can prepare new muscial files which contain only the top frequent notes
        
        new_high_frequency_music = []
        
        for notes in notes_array:
            temp = []
            for note_ in notes:
                if note_ in high_frequency_notes:
                    temp.append(note_)
            new_high_frequency_music.append(temp)
            
        new_high_frequency_music = np.array(new_high_frequency_music)
        
        # Preparing Data for input output sequences
        
        no_of_timesteps = 32
        x = []
        y = []
        
        for note_ in new_high_frequency_music:
            for i in range(0, len(note_) - no_of_timesteps,1):
                
                # Preparing input and output sequences
                input_ = note_[i:i + no_of_timesteps]
                output_ = note_[i + no_of_timesteps]
                
                x.append(input_)
                y.append(output_)
            
        x = np.array(x)
        y = np.array(y)
        
        # Now we will index each note
        
        unique_x = list(set(x.ravel()))
        x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))
        
        # Preparing integer sequences for input data
        
        # Preparing input sequences
        x_seq = []
        for i in x:
            temp = []
            for j in i:
                # Assigning unique integer to every note
                temp.append(x_note_to_int[j])
            x_seq.append(temp)
        
        x_seq = np.array(x_seq)
        
        
        # Preparing integer sequences for output data
        
        unique_y = list(set(y))
        y_note_to_int = dict((note_,number) for number, note_ in enumerate(unique_y))
        y_seq = np.array([y_note_to_int[i] for i in y])
        
        
        # Splitting the data for training, test and evaluation
        
        
        
        np.random.seed(42)
        X_train, X_test, y_train, y_test = train_test_split(x_seq,y_seq,test_size=0.2)
        
        # ### Model Building
        # **We will make 2 architectures**
        # * WaveNet
        # * LSTM
        # 
        # And experiment and evaluate both for best results
        # 
        #    #### LSTM Model
        
        
        
        K.clear_session()
        model = Sequential()
        
        # Embedding layer
        model.add(Embedding(len(unique_x),100,input_length=32,trainable=True))
        
        model.add(Conv1D(64,3,padding='causal',activation='relu'))
        model.add(Dropout(0.2))
        model.add(MaxPool1D(2))
        
        model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))
        model.add(Dropout(0.2))
        model.add(MaxPool1D(2))
        
        model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))
        model.add(Dropout(0.2))
        model.add(MaxPool1D(2))
        
        model.add(GlobalMaxPool1D())
        
        model.add(Dense(256,activation='relu'))
        model.add(Dense(len(unique_y),activation='softmax'))
        
        model.compile(loss='sparse_categorical_crossentropy',optimizer='adam')
        
        model.summary()
        
        
        mc=ModelCheckpoint('best_model.h5',monitor='val_loss',mode='min', save_best_only=True,verbose=1)
        
        history = model.fit(np.array(X_train),np.array(y_train),batch_size=128,epochs=50,validation_data=(np.array(X_test),np.array(y_test)),verbose=1,callbacks=[mc])
        
        self.X_test = X_test
        self.y_test = y_test
        self.no_of_timesteps = no_of_timesteps
        self.unique_x = unique_x
    
    def generateMusic(self,X_test,y_test,no_of_timesteps,unique_x):        
        
        # Generated from setup
        model = load_model('best_model.h5')
        
        
        ind = np.random.randint(0,len(X_test)-1)
        
        random_music = X_test[ind]
        
        predictions = []
        for i in range(10):
            
            random_music = random_music.reshape(1,no_of_timesteps)
            
            prob = model.predict(random_music)[0]
            y_pred = np.argmax(prob,axis=0)
            predictions.append(y_pred)
            
            random_music = np.insert(random_music[0],len(random_music[0]),y_pred)
            random_music = random_music[1:]
            
        print(predictions)
        
        x_int_to_note = dict((number,note_) for number, note_ in enumerate(unique_x))
        predicted_notes = [x_int_to_note[i] for i in predictions]
        
        self.convert_to_midi(predicted_notes)







